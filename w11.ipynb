{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nThey tried their best not to show it, believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nStankiewicz?  I doubt it.\\n\\nKoufax was one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n[deletia- and so on]\\n\\nI seem to have been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excuse the sheer newbieness of this post, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>==============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>\\n  Or, with no dictionary available, they cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>\\n\\nSorry to disappoint you but the Red Wings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>\\n: Can anyone tell me where to find a MPEG vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>\\nHey Valentine, I don't see Boston with any w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3451 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 corpus\n",
       "0     \\nThey tried their best not to show it, believ...\n",
       "1     \\nStankiewicz?  I doubt it.\\n\\nKoufax was one ...\n",
       "2     \\n[deletia- and so on]\\n\\nI seem to have been ...\n",
       "3     Excuse the sheer newbieness of this post, but ...\n",
       "4     ==============================================...\n",
       "...                                                 ...\n",
       "3446  \\n  Or, with no dictionary available, they cou...\n",
       "3447  \\n\\nSorry to disappoint you but the Red Wings ...\n",
       "3448  \\n: Can anyone tell me where to find a MPEG vi...\n",
       "3449                                                 \\n\n",
       "3450  \\nHey Valentine, I don't see Boston with any w...\n",
       "\n",
       "[3451 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definisikan kategori Sport, Religion, dan Technology\n",
    "categories = [\n",
    "    'comp.graphics',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey',\n",
    "    'alt.atheism',\n",
    "    'soc.religion.christian'\n",
    "]\n",
    "\n",
    "# Ambil dataset dari 20newsgroups\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "df = pd.DataFrame(dataset.data, columns=[\"corpus\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nThey tried their best not to show it, believ...</td>\n",
       "      <td>tried best show believe im surprised couldnt f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nStankiewicz?  I doubt it.\\n\\nKoufax was one ...</td>\n",
       "      <td>stankiewicz doubt koufax one two jewish hofs h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n[deletia- and so on]\\n\\nI seem to have been ...</td>\n",
       "      <td>deletia seem rather unclear asking please show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excuse the sheer newbieness of this post, but ...</td>\n",
       "      <td>excuse sheer newbieness post looking decent pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>==============================================...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>\\n  Or, with no dictionary available, they cou...</td>\n",
       "      <td>dictionary available could gain first hand kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>\\n\\nSorry to disappoint you but the Red Wings ...</td>\n",
       "      <td>sorry disappoint red wings earned victoryeasil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>\\n: Can anyone tell me where to find a MPEG vi...</td>\n",
       "      <td>anyone tell find mpeg viewer either dos window...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>\\nHey Valentine, I don't see Boston with any w...</td>\n",
       "      <td>hey valentine dont see boston world series rin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3451 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 corpus   \n",
       "0     \\nThey tried their best not to show it, believ...  \\\n",
       "1     \\nStankiewicz?  I doubt it.\\n\\nKoufax was one ...   \n",
       "2     \\n[deletia- and so on]\\n\\nI seem to have been ...   \n",
       "3     Excuse the sheer newbieness of this post, but ...   \n",
       "4     ==============================================...   \n",
       "...                                                 ...   \n",
       "3446  \\n  Or, with no dictionary available, they cou...   \n",
       "3447  \\n\\nSorry to disappoint you but the Red Wings ...   \n",
       "3448  \\n: Can anyone tell me where to find a MPEG vi...   \n",
       "3449                                                 \\n   \n",
       "3450  \\nHey Valentine, I don't see Boston with any w...   \n",
       "\n",
       "                                                cleaned  \n",
       "0     tried best show believe im surprised couldnt f...  \n",
       "1     stankiewicz doubt koufax one two jewish hofs h...  \n",
       "2     deletia seem rather unclear asking please show...  \n",
       "3     excuse sheer newbieness post looking decent pa...  \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "3446  dictionary available could gain first hand kno...  \n",
       "3447  sorry disappoint red wings earned victoryeasil...  \n",
       "3448  anyone tell find mpeg viewer either dos window...  \n",
       "3449                                                     \n",
       "3450  hey valentine dont see boston world series rin...  \n",
       "\n",
       "[3451 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if stopword\n",
    "        tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # 3. filter out words longer than 15 characters\n",
    "        tokens = [w for w in tokens if len(w) <= 20]\n",
    "        # 4. join back together\n",
    "        text = \" \".join(tokens)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "df['cleaned'] = df['corpus'].apply(lambda x: preprocess_text(x, remove_stopwords = True))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farrelmanazilin/tomatopotato/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil K-Means Clustering dengan Euclidean Distance : [2 2 1 ... 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Menggunakan Count Vectorizer untuk mengubah teks dokumen menjadi vektor fitur\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['cleaned'])\n",
    "\n",
    "# Ubah array X menjadi matriks sparse CSR\n",
    "X_sparse = csr_matrix(X)\n",
    "\n",
    "# Normalisasi matriks sparse berdasarkan Cosine Similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "X_normalized_sparse = normalize(X_sparse, norm='l2')\n",
    "\n",
    "# Melakukan K-Means Clustering dengan Euclidean Distance\n",
    "n_clusters = 3  # Ganti dengan jumlah cluster yang sesuai\n",
    "kmeans_euclidean = KMeans(n_clusters=n_clusters, random_state=0).fit(X_normalized_sparse)\n",
    "labels_euclidean = kmeans_euclidean.labels_\n",
    "\n",
    "print('Hasil K-Means Clustering dengan Euclidean Distance :', labels_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farrelmanazilin/tomatopotato/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil K-Means Clustering dengan Cosine Similarity : [1 1 2 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Menghitung Cosine Similarity\n",
    "cosine_similarities = cosine_similarity(X_normalized_sparse)\n",
    "\n",
    "# Mengisi diagonal dengan nol pada matriks jarak Cosine Similarity\n",
    "np.fill_diagonal(cosine_similarities, 0)\n",
    "\n",
    "# Melakukan K-Means Clustering dengan Cosine Similarity\n",
    "kmeans_cosine = KMeans(n_clusters=n_clusters, init='k-means++').fit(cosine_similarities)\n",
    "labels_cosine = kmeans_cosine.labels_\n",
    "\n",
    "print('Hasil K-Means Clustering dengan Cosine Similarity :', labels_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBI (Euclidean Distance): 8.819862941576746\n",
      "DBI (Cosine Similarity): 2.302873664795815\n",
      "Silhouette Score (Euclidean Distance): 0.01207831942785828\n",
      "Silhouette Score (Cosine Similarity): 0.030375310347875904\n",
      "Metode Cosine Similarity lebih baik berdasarkan DBI.\n",
      "Metode Cosine Similarity lebih baik berdasarkan Silhouette Score.\n"
     ]
    }
   ],
   "source": [
    "# Menghitung DBI dan Silhouette Score\n",
    "dbi_euclidean = davies_bouldin_score(X_normalized_sparse.toarray(), labels_euclidean)\n",
    "dbi_cosine = davies_bouldin_score(cosine_similarities, labels_cosine)\n",
    "silhouette_euclidean = silhouette_score(X_normalized_sparse.toarray(), labels_euclidean, metric='euclidean')\n",
    "silhouette_cosine = silhouette_score(pairwise_distances(cosine_similarities, metric='cosine'), labels_cosine, metric='precomputed')\n",
    "\n",
    "# Menampilkan hasil analisis\n",
    "print(\"DBI (Euclidean Distance):\", dbi_euclidean)\n",
    "print(\"DBI (Cosine Similarity):\", dbi_cosine)\n",
    "print(\"Silhouette Score (Euclidean Distance):\", silhouette_euclidean)\n",
    "print(\"Silhouette Score (Cosine Similarity):\", silhouette_cosine)\n",
    "\n",
    "# Analisis hasil berdasarkan DBI dan Silhouette Score\n",
    "if dbi_euclidean < dbi_cosine:\n",
    "    print(\"Metode Euclidean Distance lebih baik berdasarkan DBI.\")\n",
    "else:\n",
    "    print(\"Metode Cosine Similarity lebih baik berdasarkan DBI.\")\n",
    "\n",
    "if silhouette_euclidean > silhouette_cosine:\n",
    "    print(\"Metode Euclidean Distance lebih baik berdasarkan Silhouette Score.\")\n",
    "else:\n",
    "    print(\"Metode Cosine Similarity lebih baik berdasarkan Silhouette Score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA does not support sparse input. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n\u001b[0;32m----> 2\u001b[0m pca_vecs \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_normalized_sparse\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m x0 \u001b[38;5;241m=\u001b[39m pca_vecs[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m x1 \u001b[38;5;241m=\u001b[39m pca_vecs[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/tomatopotato/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/tomatopotato/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 462\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/tomatopotato/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:480\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# Raise an error for sparse input.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# This is more informative than the generic one raised by check_array.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    486\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32], ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2, random_state=42) \n",
    "pca_vecs = pca.fit_transform(X_normalized_sparse) \n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]\n",
    "df['x0'] = x0\n",
    "df['x1'] = x1\n",
    "df['euclidean'] = labels_euclidean\n",
    "df['cosine'] = labels_cosine\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean\")\n",
    "def get_top_keywords_eu(n_terms):\n",
    "    df = pd.DataFrame(Features.todense())\n",
    "    df[\"euclidean\"] = cluster_labels_euclidean\n",
    "    df = df.groupby(\"euclidean\").mean()\n",
    "    terms = cv.get_feature_names_out() \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # per ogni riga del dataframe, trova gli n termini che hanno il punteggio più alto\n",
    "            \n",
    "get_top_keywords_eu(10)\n",
    "\n",
    "cluster_map = {0: \"religion\", 1: \"technology\", 2: \"sport\"}\n",
    "df['euclidean'] = df['euclidean'].map(cluster_map)\n",
    "\n",
    "print(\"\\nCosine\")\n",
    "def get_top_keywords_cs(n_terms):\n",
    "    df = pd.DataFrame(Features.todense())\n",
    "    df[\"cosine\"] = cluster_labels_euclidean\n",
    "    df = df.groupby(\"cosine\").mean()\n",
    "    terms = cv.get_feature_names_out() \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # per ogni riga del dataframe, trova gli n termini che hanno il punteggio più alto\n",
    "            \n",
    "get_top_keywords_cs(10)\n",
    "\n",
    "cluster_map = {0: \"religion\", 1: \"technology\", 2: \"sport\"}\n",
    "df['cosine'] = df['cosine'].map(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x=df['x0'], y=df['x1'], hue='euclidean')\n",
    "plt.title('K-Means Clustering (Euclidean Distance)')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='x0', y='x1', hue='cosine')\n",
    "plt.title('K-Means Clustering (Cosine Similarity)')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
